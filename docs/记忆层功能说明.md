# 记忆层（mem0）功能说明

## 1. 设计目标
- 提供可选启用的长期记忆能力，支撑智能体跨会话的偏好与事实沉淀。
- 使用社区维护的 `mem0ai` 依赖作为核心记忆引擎，避免维护内置 fork，享受持续更新与 bug 修复。
- 通过轻量封装（`MemoryService`）向业务层提供稳定、容错的增删改查接口。

## 2. 关键组件与责任
- `app/services/memory_service.py`：封装 mem0 `Memory` 类，按需惰性初始化，并对所有操作提供失败兜底值，确保在未启用或依赖异常时不影响主流程。【F:app/services/memory_service.py†L1-L201】
- `app/api/routes/memories.py`：FastAPI 路由，暴露 `/api/memories/*` REST 接口（add/search/get/update/delete/history/delete_all），自动注入用户信息并在未启用时返回 `memory_disabled`。【F:app/api/routes/memories.py†L1-L166】
- `app/core/config.py`：集中管理记忆层开关与依赖配置项，如 `MEMORY_ENABLED`、`MEMORY_INFER`、`MEMORY_COLLECTION_NAME` 等。【F:app/core/config.py†L61-L90】
- `requirements.txt`：通过 `mem0ai==1.0.0` 引入上游依赖，确保镜像与虚拟环境可直接安装。【F:requirements.txt†L1-L30】

## 3. 初始化流程
1. 当 `settings.MEMORY_ENABLED` 为 `True` 时，`MemoryService` 首次调用会执行 `_build_config()`：
   - 构造 `VectorStoreConfig`，默认接入 Qdrant，可通过环境变量重写 URL/API Key 与向量维度。
   - 构造 `EmbedderConfig`，支持 Ollama 与 OpenAI，并根据 provider 注入必要凭据。
   - 构造 `LlmConfig`，用于可选的事实提炼（`MEMORY_INFER`）。
   - 允许自定义历史数据库路径用于 mem0 的版本化存储。
2. 按构造好的 `MemoryConfig` 初始化 mem0 的 `Memory` 对象，并缓存于 `MemoryService` 实例中。【F:app/services/memory_service.py†L21-L121】

## 4. 运行时行为
- **新增记忆**：`add_messages()` 接收消息列表、用户/Agent/Run 元数据，并自动传入 `infer` 参数（默认读取全局设置）。异常时返回 `None`。【F:app/services/memory_service.py†L60-L111】
- **检索记忆**：`search()` 默认合并调用方传入的过滤条件并补全 `user_id`，返回命中文档列表，异常时返回空列表。【F:app/services/memory_service.py†L111-L143】
- **更新/删除/查询单条**：对应方法调用底层 `Memory` 的 `update`、`delete`、`get`，异常兜底 `None`。【F:app/services/memory_service.py†L143-L175】
- **历史与批量删除**：`history()` 与 `delete_all()` 分别返回历史版本与批量删除结果，异常时返回空列表或 `None`。【F:app/services/memory_service.py†L175-L201】

## 5. 与业务集成建议
- 服务端在用户完成登录后，可通过依赖 `get_memory_service()` 访问记忆接口；若未启用记忆层会得到安全兜底，不必额外判断。【F:app/services/memory_service.py†L186-L201】
- 在 LangGraph 节点中，可于策略阶段读取 `search()` 结果补全上下文，在总结/收尾阶段调用 `add_messages()` 沉淀新事实。
- 建议所有调用均显式传入 `user_id`，并在需要区分不同 Agent 或运行实例时补充 `agent_id`、`run_id`。

## 6. 常见配置组合
| 场景 | 必选环境变量 | 说明 |
| ---- | ------------- | ---- |
| 本地开发（Ollama + 本地 Qdrant） | `MEMORY_ENABLED=true`<br>`OLLAMA_URL`<br>`QDRANT_URL` | 启动 docker-compose 时在 `.env` 中打开记忆层，使用本地模型与向量库。 |
| 云端推理（OpenAI + 托管 Qdrant） | `MEMORY_ENABLED=true`<br>`EMBEDDING_PROVIDER=openai`<br>`LLM_PROVIDER=openai`<br>`OPENAI_API_KEY`<br>`QDRANT_URL`/`QDRANT_API_KEY` | 将嵌入与 LLM 切换为 OpenAI，提供对应 Key，并配置托管 Qdrant。 |
| 关闭记忆（默认） | `MEMORY_ENABLED=false` | 所有 API 返回 `memory_disabled`，应用其余功能不受影响。 |

## 7. 调试与排错
- 首次启用时建议运行 `scripts/test_memories.py` 完成端到端验证：注册用户、写入、检索、查看历史与删除。【F:scripts/test_memories.py†L1-L107】
- 如果遇到 `memory_disabled`，检查环境变量及服务日志，确认未在 `MemoryService` 初始化之前修改配置。
- 若底层依赖抛错（网络、鉴权等），`MemoryService` 会返回兜底值；可结合日志或在本地打开 `LOG_LEVEL=debug` 追踪。

